---
title: "Measuring growth rate and reproduction number for SARS-CoV-2 variants"
output: 
  html_document:
    theme: flatly
    highlight: tango
date: "2025-03-10"
bibliography: references.bib 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# The following packages should be installed before running this code for the first time. 
library(cols4all)
library(here)
library(gridExtra)
library(scales)
library(MCMCpack)
library(EpiEstim)
library(incidence)
library(tidyverse)
library(kableExtra)
```

### Question 1

#### 1.1

**Download the dataset: download Genomes_per_week_in_England.csv from Canvas. This includes weekly counts of virus samples per lineage over time across England collected as part of Sanger Institute COG-UK.**

This dataset contains three variables across over 1500 columns: the week samples were collected, the lineage they are, and the count. The data was collected from lighthouse labs, set up to process PCR tests in England. Therefore, these data are likely from symptomatic patients.

```{r downloadDataset}

# Import COG-UK daily genomic sequence data
COGdata <- read.csv(here("data", "Genomes_per_week_in_England.csv"))

# Check the input format and display the first few rows of the data
COGdataHead <- head(COGdata)
COGdataHead %>%
  kbl() %>%
  kable_styling(full_width = FALSE, position = "center")
```

#### 1.2

**Classify major lineages: identify the following variants as major lineages: B.1.1.7 (Alpha), B.1.617.2 (Delta), BA.1, BA.2, BA.2.75, BA.4, BA.5, BA.5.3 (BQ.1), and XBB. Group all other lineages into a single category labelled as Other.**

```{r setMajorLineages}

# Specify the major lineages
major_lineages <- c("B.1.1.7", "B.1.617.2", "BA.1", "BA.2", "BA.2.75", "BA.4", "BA.5", "BA.5.3", "XBB")

# Cleaning the data
COGclean <- COGdata %>%
  # Ensure collection_date is in Date format
  mutate(date = as.Date(date)) %>%
  # Grouping all non-major lineages as 'Other' 
  mutate(lineage = ifelse(lineage %in% major_lineages, lineage, "Other")) %>%
  # Rename columns for clarity
  {colnames(.) <- c("collection_date", "major_lineage", "lineage_count"); .}

# Check the format of the columns after cleaning the data and displaying the first few rows of the data
COGcleanHead <- head(COGclean)
# Outputs the table 
COGcleanHead %>%
  kbl() %>%
  kable_styling(full_width = FALSE, position = "center")
```

#### 1.3

**Visualise the data: generate a stacked area plot showing the total counts of each major lineage over time. Generate another stacked area plot showing the frequencies (proportions) of each major lineage over time.**

```{r frequencyData}

# Calculate total counts per date
total_COG_counts <- aggregate(COGclean$lineage_count, by = list(collection_date = COGclean$collection_date), FUN = sum)
colnames(total_COG_counts) <- c("collection_date", "total_count")

# Merge total counts back into the lineage summary
COGclean <- merge(COGclean, total_COG_counts, by = "collection_date")

# Calculate frequencies
COGclean$lineage_frequency <- COGclean$lineage_count / COGclean$total_count

# Display the first few rows of the new data frame
COGcleanHead <- head(COGclean)
COGcleanHead %>%
  kbl() %>%
  kable_styling(full_width = FALSE, position = "center")
```

```{r stackedPlots, fig.height = 6, fig.width = 14, fig.align = "center", fig.cap = "Counts and frequencies of the major SARS-CoV-2 lineages over time"}

# Set a colour blind friendly colour palette
palette <- c4a("safe",10)

# Make stacked area plot for total counts of each major lineage
countPlot <- ggplot(COGclean, aes(x = collection_date, y = lineage_count, fill = major_lineage)) +
    geom_area() +  # Create a stacked area plot
    scale_fill_manual(values = palette) +  # Apply the custom palette
    labs( # Add axis labels and title 
      title = "Total counts of major lineages over time",
      x = "Collection date",
      y = "Total count",
      fill = "Major lineage"
    ) +
    theme_minimal()

# Make stacked area plot for frequencies of each major lineage
frequencyPlot <- ggplot(COGclean, aes(x = collection_date, y = lineage_frequency, fill = major_lineage)) +
    geom_area(position = "fill") +  # Create a stacked area plot
    scale_fill_manual(values = palette) +  # Apply the custom palette
    labs( # Add axis labels and title 
      title = "Frequency of major lineages over time",
      x = "Collection date",
      y = "Proportion",
      fill = "Major lineage"
    ) +
    theme_minimal()

# Printing the two plots
grid.arrange(countPlot, frequencyPlot, ncol=2, nrow =1)
```

These figures show the total counts of each major lineage and the proportion of each major lineage, using the COG-UK data. Including both of these plots is important as they are each able to show different aspects of the pandemic in more detail. The total counts is able to quantify the number of cases of each lineage at each time point, whilst the frequency plot shows the proportion of the total counts each lineage takes up, and shows the dynamics between the lineages towards the end of 2023 in more detail. In order to make this document accessible, a colour blind friendly colour palette is used throughout. The palette chosen from the cols4all package takes into account how colours would be seen by individuals with deuteranomia, protanopia and tritanopia [@Tennekes2023].

### Question 2

#### 2.1

**Visualise the COG-UK and ONS-CIS data for BA.2: plot the frequency trajectory for the BA.2 variant using both the Sanger dataset (weekly counts) and the ONS-CIS dataset (10-day bin counts from the practical).**

The code to import and clean the ONS-CIS data has not been shown here as much of it repeats the methods used in question 1, using code from the practical. It can be seen on the .rmd file.

```{r ONSdataSetup, echo = FALSE, message = FALSE}

### Import and clean the ONS data ###

# Import ONS-CIS daily genomic sequence data
ONSdata <- read.csv("https://raw.githubusercontent.com/mg878/variant_fitness_practical/main/lineage_data.csv")

# Ensure collection_date is in Date format
ONSdata$collection_date <- as.Date(ONSdata$collection_date)

# Create new data frame 
ONSsummary <- aggregate(
  ONSdata$major_lineage,
  by = list(collection_date = ONSdata$collection_date, major_lineage = ONSdata$major_lineage),
  FUN = length
)

# Rename columns for clarity
colnames(ONSsummary) <- c("collection_date", "major_lineage", "lineage_count")

# Calculate total counts per date
total_ONS_counts <- aggregate(ONSsummary$lineage_count, by = list(collection_date = ONSsummary$collection_date), FUN = sum)
colnames(total_ONS_counts) <- c("collection_date", "total_count")

# Merge total counts back into the lineage summary
ONSsummary <- merge(ONSsummary, total_ONS_counts, by = "collection_date")

# Calculate frequencies
ONSsummary$lineage_frequency <- ONSsummary$lineage_count / ONSsummary$total_count

```

```{r ONS10dayBins, echo = FALSE, message = FALSE}

### Aggregate lineage frequencies into 10-day bins ###

# Converts the Date values into numeric format, where each date is represented as the number of days since 1970-01-01 (the Unix epoch)
ONSsummary$collection_date_bin <- as.Date(
  floor(as.numeric(as.Date(ONSsummary$collection_date)) / 10) * 10, origin = "1970-01-01"
)

# Aggregate lineage counts for each 10-day bin
ONS_summary_binned <- aggregate(
  lineage_count ~ collection_date_bin + major_lineage,
  data = ONSsummary,
  FUN = sum
)

# Calculate total counts within each bin
total_ONS_counts <- aggregate(
  lineage_count ~ collection_date_bin,
  data = ONS_summary_binned,
  FUN = sum
)

# Rename columns for clarity
colnames(total_ONS_counts) <- c("collection_date_bin", "total_count") 

# Merge total counts back into the binned data
ONS_summary_binned <- merge(ONS_summary_binned, total_ONS_counts, by = "collection_date_bin")

# Recalculate frequencies
ONS_summary_binned$lineage_frequency <- ONS_summary_binned$lineage_count / ONS_summary_binned$total_count


```

```{r BA.2frequencyTrajectory, fig.height = 11, fig.width = 11, fig.align='center', echo = FALSE, message = FALSE, fig.cap = "Daily trajectories of BA.2 for the ONS-CIS and COG-UK datasets"}

# Filter COG data for BA.2
COG_daily_trajectories <- subset(
  COGclean,
  major_lineage %in% c("BA.2")
)

# Filter ONS data for BA.2
ONS_daily_trajectories <-subset(
  ONS_summary_binned,
  major_lineage %in% c("BA.2")
)


# COG Data BA.2 frequency trajectory plot
COGBA2plot <- ggplot(COG_daily_trajectories, aes(x = collection_date, y = lineage_frequency)) +
  geom_line(linewidth = 1, color = "#6699CC") +  # Connect the points with lines
  geom_point(size = 2, alpha = 0.7) +  # Add points for daily frequencies
  labs( # Labels the axes and title 
    title = "COG Frequency trajectories of BA.2",
    x = "Collection date",
    y = "Proportion",
  ) +
  theme_minimal()

# ONS data BA.2 frequency trajectory plot
ONSBA2plot <- ggplot(ONS_daily_trajectories, aes(x = collection_date_bin, y = lineage_frequency)) +
  geom_line(linewidth = 1, color = "#661100") +  # Connect the points with lines
  geom_point(size = 2, alpha = 0.7) +  # Add points for daily frequencies
  labs( # Labels the axes and title 
    title = "ONS Frequency trajectories of BA.2",
    x = "Collection date",
    y = "Proportion",
  ) +
  theme_minimal()

# ONS and COG data on the same plot
BA2ONSCOGplot <- ggplot(COG_daily_trajectories, aes(x = collection_date_bin, y = lineage_frequency)) +
  geom_line(data = COG_daily_trajectories, # Adds a line for the COG data
            linewidth = 1, 
            aes(color = "COG Data", x = collection_date )) + 
  geom_line(data = ONS_daily_trajectories, 
            linewidth = 1, 
            aes(color = "ONS Data")) + # Adds another line for the ONS data
  scale_color_manual(name = "Dataset", # Adds a legend  
                     values = c("COG Data" = "#6699CC", "ONS Data" =  "#661100")) +
  labs( # Labels the legend, x and y axes and title
    title = "Frequency trajectories of BA.2",
    x = "Collection date",
    y = "Proportion",
    color = "Dataset") +
  theme_minimal() +
  theme(legend.position.inside = c(0.225, 0.75), # positions the legend to not interfere with the data
        legend.background = element_rect(fill = "white", color = "black")) # Adds a background to the legend
  
# ONS and COG data on the same plot zoomed in
ZoomedBA2ONSCOGplot <- ggplot(COG_daily_trajectories, aes(x = collection_date_bin, y = lineage_frequency)) +
  geom_line(data = COG_daily_trajectories, # Adds a line for the COG data
            linewidth = 1, 
            aes(color = "COG Data" , x = collection_date)) + 
  geom_line(data = ONS_daily_trajectories, # Adds another line for the ONS data
            linewidth = 1, 
            aes(color = "ONS Data")) +
  scale_color_manual(name = "Dataset",  # Adds a legend
                     values = c("COG Data" = "#6699CC", "ONS Data" =  "#661100")) +
  labs( # Labels the legend, x and y axes and title
    title = "Englarged frequency trajectories of BA.2",
    x = "Collection date",
    y = "Proportion",
    color = "Dataset") +
  theme_minimal() +
  theme(legend.position.inside = c(0.225, 0.75), # positions the legend to not interfere with the data
        legend.background = element_rect(fill = "white", color = "black"))+ # Adds background to the legend
  coord_cartesian(xlim = as.Date(c("2022-01-01", "2022-06-30"))) # Sets the x axis to certain dates

# Outputting the plots 
grid.arrange(COGBA2plot, ONSBA2plot, BA2ONSCOGplot, ZoomedBA2ONSCOGplot, ncol=2, nrow =2)

```

#### 2.2

**Analysis: compare the two trajectories. Is there a difference in the timing of BA.2’s rise and when it reaches fixation? Reflect on potential reasons for these differences (sampling strategies and geographical or temporal biases in data collection)?**

The two trajectories are almost identical, with BA.2 first appearing at the start of 2022, rising to fixation around 3 months later. The ONS peak for BA.2 is slightly ahead of the COG data peak. One reason for this could be that the ONS data was collected from individuals in the community, whether symptomatic or not, whilst the COG data was collected from patients. This means that the ONS data would have picked up cases before they became symptomatic, whilst the COG data may have a small delay due to the time taken for cases to become symptomatic.

Another reason for the small difference in timing could be due to geographical biases. The COG-UK data consists of pillar 2 samples, collected from lighthouse labs across the UK, located in key strategic sites. The data excludes pillar 1 samples, from other locations, which were sequenced by local universities or NHS trusts instead of the Wellcome Sanger Institute [@nicholls2021climb]. Meanwhile, the ONS-CIS dataset was collected throughout the UK [@steel2022coronavirus]. If BA.2 first spread in particular areas which were not located close to hte lighthouse lab locations it could result in it being picked up on the ONS-CIS data before the COG-UK data.

### Question 3

**Using the Sanger dataset, determine which variant—B.1.617.2, BA.1, or BA.2—reached fixation the fastest and exhibited the highest selective advantage under a logistic growth model. Use weekly counts to measure the selective advantage (𝑠)**

```{r selectiveAdvantage, message = FALSE}

# Setting up the logistic growth equation function
logistic_growth <- function(t, s, f0) {
  (f0 * exp(s * t)) / (1 + f0 * (exp(s * t) - 1))
}


# Setting up the selectiveAdvantage function
selectiveAdvantage <- function(lineage, title1, title2, startDate, endDate, colour) {
  
  variant_data <- subset(COGclean, major_lineage == lineage)
  
  dailyFrequencyPlot <- ggplot(variant_data, aes(x = collection_date, 
                                                 y = lineage_frequency)) +
    geom_line(color = colour, linewidth = 1) +  # Static color assigned outside aes()
    geom_point(size = 2, alpha = 0.7, color = colour) +  # Static color for points
    labs(
      title = title1,
      x = "Collection Date",
      y = "Proportion"
    ) +
    theme_minimal()
  
  
  # Subset data to only include the increasing trajectory for Delta
  variant_growth_phase <- variant_data[
    variant_data$collection_date >= as.Date(startDate) & variant_data$collection_date <= as.Date(endDate),
  ]
  
  # Fit the logistic model using nls
  nls_fit <- nls(
    lineage_frequency ~ logistic_growth(as.numeric(collection_date - min(collection_date)), s, f0),
    data = variant_growth_phase,
    start = list(s = 0.1, f0 = min(variant_growth_phase$lineage_frequency))  # Initial guesses
  )
  
  # Extract fitted growth rate
  growth_rate <- coef(nls_fit)["s"]
  
  # Generate a smooth sequence of dates for plotting the logistic curve
  smooth_dates <- seq(min(variant_growth_phase$collection_date),
                      max(variant_growth_phase$collection_date), by = "1 day")
  
  # Calculate predicted frequencies for smooth (continuous) dates 
  smooth_predictions <- data.frame(
    collection_date = smooth_dates,
    predicted_frequency = logistic_growth(as.numeric(smooth_dates - min(variant_growth_phase$collection_date)),
                                          coef(nls_fit)["s"], coef(nls_fit)["f0"])
  )
  
  # Visualise the actual data points and the smooth logistic fit
  logisticFitPlot <- ggplot(variant_growth_phase, aes(x = collection_date)) +
    geom_point(aes(y = lineage_frequency), color = "black", size = 2, alpha = 0.7) +
    geom_line(data = smooth_predictions, aes(x = collection_date, y = predicted_frequency),
              color = colour, size = 1) +
    annotate(
      "text", 
      x = min(variant_growth_phase$collection_date, na.rm = TRUE) + 0.2 * # Sets the location of the text to be 20% of the way across the plot
        (max(variant_growth_phase$collection_date, na.rm = TRUE) - 
         min(variant_growth_phase$collection_date, na.rm = TRUE)), 
      y = 0.8, 
      label = paste0("s= ", round(growth_rate, 4)), 
      color = colour, 
      size = 5
    ) +
    labs( # Labels the axes and title 
      title = title2,
      x = "Collection date",
      y = "Frequency"
    ) +
    theme_minimal()
  
  grid.arrange(dailyFrequencyPlot, logisticFitPlot, ncol=2, nrow =1)
}
```

```{r deltaSelectiveAdvantage, echo = FALSE, message = FALSE, fig.height = 5, fig.width = 10, fig.align='center', fig.cap = "The logistic growth fit for the B.1.617.2 variant over time"}

# Running the selectiveAdvantage function for the B.1.617.2 variant 
selectiveAdvantage(lineage = "B.1.617.2", 
                   title1 = "Daily Frequency Trajectories of B.1.617.2", 
                   title2 = "Logistic growth fit for B.1.617.2 variant frequency",
                   startDate = "2021-04-01", 
                   endDate = "2021-07-12",
                   colour = "#CC6677")  # Colours match those used for variants in Q1
```

```{r BA1selectiveAdvantage, echo = FALSE, message = FALSE, fig.height = 5, fig.width = 10, fig.align='center', fig.cap = "The logistic growth fit for the BA.1 variant over time"}

# Running the selectiveAdvantage function for the BA.1 variant 
selectiveAdvantage("BA.1", 
                   "Daily Frequency Trajectories of BA.1", 
                   "Logistic growth fit for BA.1 variant frequency",
                   "2021-11-25", 
                   "2022-01-10",
                   "#DDCC77") # Colours match those used for variants in Q1
```

```{r BA2selectiveAdvantage, echo = FALSE, message = FALSE, fig.height = 5, fig.width = 10, fig.align='center', fig.cap = "The logistic growth fit for the BA.2 variant over time"}

# Running the selectiveAdvantage function for the BA.2 variant 
selectiveAdvantage("BA.2", 
                   "Daily Frequency Trajectories of BA.2", 
                   "Logistic growth fit for BA.2 variant frequency",
                   "2022-01-01", 
                   "2022-04-20",
                   "#117733") # Colours match those used for variants in Q1
```

From these data, BA.1 has the highest growth rate of s = 0.2598, followed by B.1.617.2 with s = 0.1267, and then BA.2 with s = 0.1038. However, as these variants did not all compete against each other simultaneously, this cannot be used alone as a measure to compare their fitness. The envrionment at the time must also be taken into account, with the potential of cross immunity between variants, measures such as social distancing, and the introduction of vaccination also affecting the growth rate of the variants over time.

### Question 4

#### 4.1

**Load the dataset: download and load delta-d2.rds from Canvas. This dataset contains an anonymised line list of individuals with a sequenced Delta sample from various regions in England, collected as part of the COG-UK. (hint: remove rows for which there is no associated phecname subset(data, phecname != ""))**

```{r loadDeltaDataset}

# Loading in the dataset
deltaDataRaw <- readRDS(here("data", "delta-d2.rds"))

# Cleaning the data
deltaDataClean <- deltaDataRaw %>%
  # Rename columns for clarity
  rename(
    collection_date = date,
    region = phecname,
    delta = Delta) %>%
  # Ensure collection_date is in Date format
  mutate(collection_date = as.Date(collection_date)) %>%
  # Remove rows with no associated region 
  filter(region != "") %>%
  # Remove the time column. dplyr::select is needed due to the conflict with select() in other packages
  dplyr::select(-time)

# Outputs the first 6 rows of the dataset to check structure  
deltaDataCleanHead <- head(deltaDataClean)
deltaDataCleanHead %>%
  kbl() %>%
  kable_styling(full_width = FALSE, position = "center")
```

This dataset consists of over 229,000 rows. Each one has a collection date, which is daily, not weekly as with the previous COG-UK data set, whether the sample is or isn't a Delta variant, and the region it was collected in. England has been split into 9 regions for this: East Midlands, East of England, London, North East, North West, South East, South West, West Midlands and Yorkshire and Humber.

#### 4.2

**Analyse and visualise the data: (i) plot Delta frequencies by region; for each region, plot the frequency of Delta over time. Use distinct colours or facets to differentiate between regions. (ii) Fit logistic growth for each region; for each region, fit a logistic growth model to the frequency data. Overlay the logistic growth curves onto the frequency trajectories for each region.**

The code for calculating the counts and sorting the data into 7 day bins has been excluded from the html file as the same methods are used as in Q1. The full code can be seen on the rmd file.

```{r calculatingCountData, echo = FALSE, message = FALSE}
# Create new data frame 
deltaSummary <- aggregate(
  deltaDataClean$delta,
  by = list(
    collection_date = deltaDataClean$collection_date,
    delta = deltaDataClean$delta,
    region = deltaDataClean$region),
  FUN = length
)

# Rename columns for clarity
colnames(deltaSummary) <- c("collection_date", "delta", "region", "lineage_count")

# Calculate total counts per date
total_delta_counts <- aggregate(
  deltaSummary$lineage_count, by = list(
    collection_date = deltaSummary$collection_date,
    region = deltaSummary$region), 
  FUN = sum)
colnames(total_delta_counts) <- c("collection_date", "region", "total_count")

# Merge total counts back into the lineage summary
deltaSummary <- merge(deltaSummary, total_delta_counts, by = c("collection_date", "region"))

# Calculate frequencies
deltaSummary$lineage_frequency <- deltaSummary$lineage_count / deltaSummary$total_count
```

```{r binnedData, echo = FALSE, message = FALSE}

# Aggregate lineage frequencies into 7-day bins
# Converts the Date values into numeric format, where each date is represented as the number of days since 1970-01-01 (the Unix epoch)
deltaSummary$collection_date_bin <- as.Date(
  floor(as.numeric(as.Date(deltaSummary$collection_date)) / 7) * 7, origin = "1970-01-01"
)

# Aggregate lineage counts for each 7-day bin
deltaSummaryBinned <- aggregate(
  lineage_count ~ collection_date_bin + delta + region,
  data = deltaSummary,
  FUN = sum
)

# Calculate total counts within each bin
totalDeltaBinCounts <- aggregate(
  lineage_count ~ collection_date_bin + region,
  data = deltaSummaryBinned,
  FUN = sum
)
colnames(totalDeltaBinCounts) <- c("collection_date_bin", "region", "total_count")  # Rename for clarity

# Merge total counts back into the binned data
deltaSummaryBinned <- merge(deltaSummaryBinned, totalDeltaBinCounts, by = c("collection_date_bin", "region"))

# Recalculate frequencies
deltaSummaryBinned$lineage_frequency <- deltaSummaryBinned$lineage_count / deltaSummaryBinned$total_count

```

```{r logisticModel, fig.align = "center", fig.cap = "The frequencies and logisitic model fit of the SARS-CoV-2 Delta variant over time"}
# Subsetting the data for Delta positive tests
deltaPositive <- subset(deltaSummaryBinned, delta == TRUE)

# List of unique regions
regions <- c("East Midlands", "East of England", "London", "North East", "North West", "South East", "South West", "West Midlands", "Yorkshire and Humber")

# Empty list to store model parameters for Q4.3
region_parameters <- data.frame(region = character(), f0 = numeric(), s = numeric(), stringsAsFactors = FALSE)

# Empty list to store logistic fit results for each region
all_smooth_predictions <- list()

# Loop for each region
for (r in regions) {
  
  # Subset data for the current region
  variant_growth_phase <- subset(deltaPositive, region == r)

  # Fit the logistic model using nls
  nls_fit <- nls(
    lineage_frequency ~ logistic_growth(as.numeric(collection_date_bin - min(collection_date_bin)), s, f0),
    data = variant_growth_phase,
    start = list(s = 0.1, f0 = min(variant_growth_phase$lineage_frequency))
  )
  
   # Extract parameters 
  growth_rate <- coef(nls_fit)["s"]
  initial_frequency <- coef(nls_fit)["f0"]
  
  # Store the parameters in the dataframe for Q4.3
  region_parameters <- rbind(region_parameters, 
                             data.frame(region = r, 
                                        f0 = initial_frequency,
                                        s = growth_rate))
  
  # Generate a smooth sequence of dates for plotting the logistic curve
  smooth_dates <- seq(min(variant_growth_phase$collection_date_bin),
                      max(variant_growth_phase$collection_date_bin), by = "1 day")
  
  
  # Calculate predicted frequencies for smooth (continuous) dates 
  smooth_predictions <- data.frame(
    collection_date_bin = smooth_dates,
    predicted_frequency = logistic_growth(
      as.numeric(smooth_dates - min(variant_growth_phase$collection_date_bin)),
      coef(nls_fit)["s"], coef(nls_fit)["f0"]),
      region = r
    )
  
 
  # Store model predictions for the current region
  all_smooth_predictions[[r]] <- smooth_predictions
}

# Combine all of the region predictions into one data frame
all_smooth_predictions <- do.call(rbind, all_smooth_predictions)

# Plot data
ggplot(deltaPositive, aes(x = collection_date_bin, y = lineage_frequency)) +
  geom_line(data = all_smooth_predictions, # Adds a line for the logistic growth model 
            aes(y = predicted_frequency, 
                color = "Logistic Model"), size = 1, alpha = 0.5) + 
  geom_line(aes(color = "Frequencies")) + # Adds a line for the frequencies 
  facet_wrap(~region) + # Splits the graph into facets by region
  scale_color_manual(name = "Legend", # Sets up the colours for the legend 
                     values = c("Frequencies" = "black", "Logistic Model" = "#CC6677")) +
  labs(title = "Delta Frequencies and logistic model fit by Region", # Labels the title and axes
       x = "Collection date", 
       y = "Frequency") +
  theme_minimal()
```

These plots show the frequencies of Delta in each region, shown in black, as well as the logistic growth curves, overlaid in red. As the start and end dates of when Delta first emerged and when it reached fixation were very similar for each region, the same dates were used for all. These plots show that for the most part, the logistic growth model fits the frequency data. In some regions, it appears to be a less good fit, for example in the South West and North East, where growth was not fully logarithmic. Therefore, the results and parameters from the model should be treated with caution for these regions.

#### 4.3

**Interpretation: (i) Identify the region with the fastest Delta outbreak; based on your analysis, determine which region had the most rapidly growing outbreak of Delta (highest 𝑠). Identify the region where Delta had the earliest rise in frequencies (highest 𝑓0). Discuss why the timing of Delta’s emergence differed between regions. (ii) Could Delta’s growth across regions be associated with a founder effect? Explain what a founder effect is and evaluate whether the observed data supports or refutes this hypothesis.**

```{r parameterPlots, fig.width = 11, fig.height = 5, fig.align='center', fig.cap = "The parameters s and f0 of the logisitc growth model for the SARS-CoV-2 Delta variant in differnet regions of England"}

# Plotting s per region
sPlot <- ggplot(region_parameters, aes(x = region, y = s, fill = region)) + 
  geom_col() +
  scale_fill_manual(values = palette) +  # Apply the custom palette
    labs(
      title = "Growth rate (s) per region",
      x = "Region",
      y = "s") +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_x_discrete(labels = label_wrap(10))

# Plotting f0 per region
f0Plot <- ggplot(region_parameters, aes(x = region, y = f0, fill = region)) + 
  geom_col() +
  scale_fill_manual(values = palette) +  # Apply the custom palette
    labs(
      title = "f0 per region",
      x = "Region",
      y = "f0") +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_x_discrete(labels = label_wrap(10))

# Displaying the plots 
grid.arrange(sPlot, f0Plot, ncol=2, nrow =1)

```

s is the selection coefficient, the odds of a variant appearing in a random sample in a large population over time will increase at a rate of s per generation. This can be used to represent the growth rate of a variant. From these data, s is greatest in the West Midlands, at 0.155. Meanwhile, regions with the highest f0 have the earliest rise in frequencies. From these data, this occurs in the South West, where f0 is 0.040. Whilst these data are useful in understanding the dynamics of the Delta variant, a logistic growth model may not be right for this data. It assumes a constant population size and fixed generation time, which may vary based on public health measures such as lockdowns and social distancing, alongside a constant selective advantage, which also cannot be assumed due to immunity building in the host population. Despite this, logistic growth models are robust to changes in the rate of reporting and any sampling lags [@volz2023fitness], which is important to consider especially early in the pandemic when recording systems were still being set up, but it doesn't give all of the information needed to determine if a variant is a public health threat. Other models, such as Bayesian heirarhcial models or generalised multinomal logistic regression models may be appropriate instead.

Founder effects are the reduction in genetic diversity due to the colonisation of a region by a small, subset of a few viral variants from a larger population [@volz2023fitness]. This can often mask the effects of natural selection when looking at a logistic growth model. The extent to which a founder effect occurs depends on the size of the founder population that infects a new region, and the rate of exponential growth in that new region. As the delta variant was introduced to England over 1000 times [@mccrone2022context], the founder effect is likely reduced, although the impact this has on regions depends on if these introductions were evenly spread throughout England, or were concentrated in one area. The large range in f0 between regions could imply a founder effect. To fully ascertain this, more data on the genetic sequences and their similarity of the delta variant in different regions would be needed. \### Question 5

#### 5.1

**Estimate the true incidence of Delta: Up to this point, we have relied on the number of PCR-positive tests sent to the Sanger Institute for sequencing to estimate the growth rate of variants. However, does this approach accurately reflect the true incidence of Delta infections in England? Explain how it is different from the incidence.**

The number of PCR positive tests sent to the Wellcome Sanger Institute is a sample of all Delta cases in the UK, and is therefore not the true incidence. Not all sequences were sent to the Sanger Institute, the datasets analysed so far contain pillar 2 sampling. Many cases were also sampled by pillar 1 sampling, occuring in local universities or NHS trusts [@nicholls2021climb]. This introduces a geographical bias, with different regions likely sending in a different proportion of positive PCR tests to the Sanger Institute.

Futhermore, this data only reflects the people who came forward to undergo PCR testing. This will bias the sampling if some communitites were less likely to undergo testing than others, for example not being able to afford the time off work. It also excludes asymptomatic cases, which are unlikley to have been picked up by PCR testing and sent into Sanger. Asymptomatic infections could make up as much as 40% of cases [@jamanetworkopen2021]. If the proportion of asymptomatic infections varied between SARS-CoV-2 variants, only looking at symptomatic cases could affect the conclusions made about growth rates. Therefore, whilst the Sanger Institute sequenced a large quantity of positive PCR tests, their sampling means they may not accurately reflect the true incidence of Delta infections in England.

**To obtain a more representative estimate of the true number of Delta infections, multiply the proportion of Delta sequences in England (from the Sanger dataset) by the daily (7-day averaged) COVID-19 case counts in England provided in the daily-new-confirmed-covid-19-cases.csv dataset on Canvas.**

**Note that while the daily COVID-19 case counts are reported on a daily basis, the proportion of Delta sequences from the Sanger dataset is calculated weekly. For this task, use the same weekly proportion of Delta for every day within each 7-day interval of the daily case counts.**

**Plot estimated daily cases of Delta and weekly number of Delta sequences from Sanger. Reflect on why the two counts are different from each other.**

```{r calculateAndPlotIncidence, message = FALSE, warning = FALSE, fig.align='center', fig.width = 7, fig.height = 5, fig.cap = "The estimated daily counts and Sanger Institude sequencing counts of Delta incidence over time"}
library(tidyverse)

# Loading in the data
dailyCases <- read.csv(here("data", "daily-new-confirmed-covid-19-cases.csv"))

# Cleaning dailyCases so it can be merged with COGclean
dailyCases <- dailyCases %>%
  # filter out dates outside of the range of the COG dataset
  filter(date >= as.Date("2020-09-05") & date <= as.Date("2023-02-11")) %>% 
  # Rename columns for clarity 
  rename(cases = cases_sevendayaveraged) %>% 
  # Aggregate lineage frequencies into 7-day bins
  mutate(
    date = as.Date(date),
    weeklyDate = as.Date(floor((as.numeric(date) - as.numeric(as.Date("2020-09-05"))) / 7) * 7 + as.numeric(as.Date("2020-09-05")), 
    origin = "1970-01-01"))
 
# Filtering COGclean, merging it with dailyCases and calculate deltaIncidence
dailyIncidence <- COGclean %>% 
  # Filtering the dataset to only include Delta
  filter(major_lineage == "B.1.617.2") %>% 
  # Removing unneeded columns
  dplyr::select(-c(major_lineage, total_count)) %>% 
  # Renaming column for consistency with the dailyCases dataset
  rename(weeklyDate = collection_date) %>% 
  # Merging this dataset with dailyCases
  left_join(dailyCases, by = "weeklyDate") %>% 
  # Calculating daily incidence of Delta 
  mutate(deltaIncidence = lineage_frequency * cases)

# Plotting estimated daily cases, and weekly number of delta sequences 
ggplot(dailyIncidence, aes(x = weeklyDate)) +
  geom_line(aes(y = lineage_count, # Plots weekly no. delta seqeunces
                color = "Sanger Sequences")) + 
  geom_line(aes(x = date, # Plots estimated daily delta cases
                y = deltaIncidence, 
                color = "Estimated Daily Cases")) +
  scale_x_date(limits = c(as.Date("2021-04-01"), as.Date("2022-04-01"))) +
  scale_color_manual( # Adds the legend 
    name = "Legend", 
    values = c(
      "Sanger Sequences" =  "#6699CC",
      "Estimated Daily Cases" =  "#661100")) +
  labs( # Labels the axes and title
    title = "Lineage Count and Delta Incidence Over Time",
    x = "Date",
    y = "Count"
  ) +
  theme_minimal() +
  theme(legend.background = element_rect(fill = "white", color = "black")) # Adds background to the legend)


```

The two counts, although showing a similar pattern of a rise and fall in cases between April 2021 and March 2022, vary from each other. On the whole, the Sanger counts are fewer, apart from very early on in the outbreak of Delta. One reason for this could be that the Sanger data used in this assessment only includes sequences from the lighthouse labs. Other locations, such as local universities and NHS trusts were also used to sequence data tests, which was included in the estimated daily cases data, increasing the count.

The estimated daily case counts include additional peaks in August 2021 and November 2021 which aren't seen in the Sanger sequences. The additional sequences included in the daily case estimates across the UK means if there was variation in the number of Delta cases per region, or a local peak in the number of cases, this would be represented better in the daily estimate data.

#### 5.2

**Measure 𝑅𝑡: Using the estimated daily Delta case counts, calculate the time-varying reproduction number (𝑅𝑡) using the same method as in the practical (hint: use this time range: “2021-04-23” to “2021-11-01” for measurement).**

**Compare your 𝑅𝑡 estimate to the one calculated during the practical using the ONS-CIS dataset.**

**Reflect on whether the two estimates differ significantly. Which estimate do you consider more reliable, and why? (hint: consider factors such as the sampling strategy used by the different datasets and the representativeness of the sequencing data for actual infections.)**

```{r Rt, fig.height = 5, message = FALSE, warning = FALSE, fig.width = 10, fig.align='center', fig.cap = "The Rt estimates for the Delta variant, based on the ONS-CIS and COG-UK datasets"}

### Calculating Rt for the Sanger data

# Specify start and end dates
start_date <- as.Date("2021-04-23")
end_date <- as.Date("2021-11-01")

# Filter delta_daily_counts for the specified date range
filtered_delta_sanger <- subset(
  dailyIncidence,
  date >= start_date & date <= end_date
)

# Prepare data for estimate_R
sanger_incidence_data <- data.frame(
  dates = filtered_delta_sanger$date,
  I = filtered_delta_sanger$deltaIncidence  # Use lineage_count as daily incidence
)

# Define serial interval parameters
serial_interval <- list(mean_si = 4.1, std_si = 2.8)

# Estimate R_t
rt_results_sanger <- estimate_R(
  incid = sanger_incidence_data,
  method = "parametric_si",
  config = make_config(serial_interval)
)


### Calculating Rt for the ONS dataset - as done in the practical 

# Specify start and end dates
# The input incidence data for EpiEstim cannot include missing dates and case counts 
start_date <- as.Date("2021-06-01")
end_date <- as.Date("2021-11-01")

# Filter daily counts for Delta variant directly from lineage_summary
ONS_delta_daily_counts <- subset(
  ONSsummary,
  major_lineage == "B.1.617.2"  # *Filter for Delta variant*
)

# Filter delta_daily_counts for the specified date range
filtered_delta_ONS <- subset(
  ONS_delta_daily_counts,
  collection_date >= start_date & collection_date <= end_date
)

# Prepare data for estimate_R
incidence_data <- data.frame(
  dates = filtered_delta_ONS$collection_date,
  I = filtered_delta_ONS$lineage_count  # Use lineage_count as daily incidence
)

# Estimate R_t
rt_results_ONS <- estimate_R(
  incid = incidence_data,
  method = "parametric_si",
  config = make_config(serial_interval)
)

### Plotting the Rt estimates

# Create the Sanger data Rt plot 
rtSangerPlot <- plot(rt_results_sanger, what = "R", legend = FALSE) +
    plot(rt_results_ONS, what = "R", legend = FALSE)$layers[[1]] +
  labs(
    title = expression("Sanger Time-varying reproduction number" ~ (R[t]) ~ "for Delta"),
    x = "Date",
    y = expression("Reproduction number" ~ (R[t]))
  )

# Create the ONS data Rt plot
rtONSPlot <- plot(rt_results_ONS, what = "R", legend = FALSE) +
    plot(rt_results_ONS, what = "R", legend = FALSE)$layers[[1]] +
  labs(
    title = expression("ONS Time-varying reproduction number" ~ (R[t]) ~ "for Delta"),
    x = "Date",
    y = expression("Reproduction number" ~ (R[t]))
  )

# Display both plots 
grid.arrange(rtONSPlot, rtSangerPlot, ncol=2, nrow =1)
```

In making these plots, the serial interval parameters used were previously estimated for Delta [@Backer2022].

*R~t~* is the reproduction number at time *t*, unlike *R~0~* it represents a population of both naive and immune individuals, instead of fully naive individuals at the beginning of an epidemic [@estimates2020]. Calculating *R~t~* accurately is vital to give information on the impact of control measures, and how an epidemic is likely to look going forwards. 

One advantage of the ONS-CIS data is that it is more randomly selected than the COG-UK data, which only contains sequences from positive PCR tests. However, the sampling is not completely random, coming from a list of people who have participated in previous ONS studies. Furthermore, the sample size is more consistent for the ONS-CIS dataset, whilst the COG-UK dataset varies in size depending on the current count and current testing facility capacities.

Both datasets give very similar estimates for *R~t~*  over time. They both reflect similar trends, including a *R~t~* < around August. The uncertainty is a lot smaller for the COG-UK dataset. This could be because a larger number of positive cases are being sampled, unlike the ONS-CIS data, which samples many people who do not currently have SARS-CoV-2. For this reason, the quantity of data, despite not being randomly selected, makes me consider the COG-UK estimate more reliable. 

\### References
